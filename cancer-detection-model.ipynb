{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import methods as md\n",
    "#train_labels, id_label_map, get_id_from_file_path(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_files = glob('./dataset/training/*.tif')\n",
    "train, val = train_test_split(labeled_files, test_size=0.2, random_state=101010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(list_files, id_label_map, batch_size, augment=False):\n",
    "    keras_gen = ImageDataGenerator(\n",
    "                    rotation_range=10,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    zoom_range=0.2,\n",
    "                    shear_range=5)\n",
    "    while True:\n",
    "        shuffle(list_files)\n",
    "        for batch in chunker(list_files, batch_size):\n",
    "            X = [cv2.imread(x) for x in batch]\n",
    "            Y = [id_label_map[md.get_id_from_file_path(x)] for x in batch]\n",
    "            if augment:\n",
    "                X = [keras_gen.random_transform(x) for x in X]\n",
    "            X = [preprocess_input(x.astype(np.float32)) for x in X]\n",
    "                \n",
    "            yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classif_mobilenet2():\n",
    "    inputs = Input((96, 96, 3))\n",
    "    base_model = MobileNetV2(include_top=False, input_shape=(96, 96, 3))#, weights=None\n",
    "    x = base_model(inputs)\n",
    "    out1 = GlobalMaxPooling2D()(x)\n",
    "    out2 = GlobalAveragePooling2D()(x)\n",
    "    out3 = Flatten()(x)\n",
    "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
    "    model = Model(inputs, out)\n",
    "    model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "9412608/9406464 [==============================] - 3s 0us/step\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 96, 96, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_96 (Model)     (None, 3, 3, 1280)   2257984     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 1280)         0           mobilenetv2_1.00_96[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1280)         0           mobilenetv2_1.00_96[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 11520)        0           mobilenetv2_1.00_96[1][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14080)        0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14080)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "3_ (Dense)                      (None, 1)            14081       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,272,065\n",
      "Trainable params: 2,237,953\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model_classif_mobilenet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "h5_path = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5500/5500 [==============================] - 6807s 1s/step - loss: 0.3295 - acc: 0.8711 - val_loss: 0.3049 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88736, saving model to model.h5\n",
      "Epoch 2/2\n",
      "5500/5500 [==============================] - 6590s 1s/step - loss: 0.2106 - acc: 0.9195 - val_loss: 0.2475 - val_acc: 0.9157\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.88736 to 0.91565, saving model to model.h5\n",
      "Epoch 1/6\n",
      "2750/2750 [==============================] - 6635s 2s/step - loss: 0.1574 - acc: 0.9411 - val_loss: 0.1929 - val_acc: 0.9341\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.91565 to 0.93407, saving model to model.h5\n",
      "Epoch 2/6\n",
      "  17/2750 [..............................] - ETA: 1:49:59 - loss: 0.1652 - acc: 0.9390"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    data_gen(train, md.id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, md.id_label_map, batch_size),\n",
    "    epochs=2, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)\n",
    "batch_size=64\n",
    "history = model.fit_generator(\n",
    "    data_gen(train, md.id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, md.id_label_map, batch_size),\n",
    "    epochs=6, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)\n",
    "    \n",
    "model.compile(optimizer=Adam(0.00001), loss=binary_crossentropy, metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "    data_gen(train, md.id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, md.id_label_map, batch_size),\n",
    "    epochs=6, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(h5_path)\n",
    "\n",
    "preds = []\n",
    "ids = []\n",
    "\n",
    "for batch in chunker(test_files, batch_size):\n",
    "    X = [preprocess_input(cv2.imread(x).astype(np.float32)) for x in batch]\n",
    "    ids_batch = [get_id_from_file_path(x) for x in batch]\n",
    "    X = np.array(X)\n",
    "    preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
    "    preds += preds_batch\n",
    "    ids += ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':ids, 'label':preds})\n",
    "df.to_csv(\"baseline_mobilenet_v2.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
